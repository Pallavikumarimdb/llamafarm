# Default context sizes for GGUF models
# Patterns use Unix shell-style wildcards (*, ?, [seq])
# More specific patterns should be listed first

# Memory usage factor for computing max context size
# 0.8 = use 80% of available memory (aggressive but safe for most systems)
memory_usage_factor: 0.8

model_defaults:
  # Exact model matches (highest priority)
  - pattern: "unsloth/Qwen2.5-Coder-1.5B-Instruct-GGUF"
    n_ctx: 32768
    notes: "Qwen 2.5 supports 32k context"

  - pattern: "unsloth/gpt-oss-*"
    n_ctx: 8192
    notes: "GPT-OSS models default to 8k context"

  # Wildcard patterns (lower priority)
  - pattern: "*Qwen2.5*"
    n_ctx: 32768
    notes: "Qwen 2.5 family supports 32k context"

  - pattern: "*Llama-3*"
    n_ctx: 8192
    notes: "Llama 3 family default"

  - pattern: "*Mistral*"
    n_ctx: 32768
    notes: "Mistral models support 32k context"

  - pattern: "*"
    n_ctx: 4096
    notes: "Fallback default for unknown models"
